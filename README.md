# Sprint 1: TypeScript CSV

### Task C: Proposing Enhancement

- #### Step 1: Brainstorm on your own.
    One: There may be errors in handling data seperation. This could include a location like Providence, RI, where the location is actually just one data type.

    Two: Handling strings vs. handling integers given age. Making sure that both work as well as additional data types. 

    Three: Handling different edge cases, like having tests with quotes, commas, and empty columns. 

    Four: Error reporting, provide clear error message handling that explains exactly what the issue is rather than just a broad undefined message. This way, the people on the other side can easily understand the issue and how to approach these challenges.

- #### Step 2: Use an LLM to help expand your perspective.
    Asking LLM about the most pressing CSV parsing problems to consider, concise summary: 

    Input Flexibility: 
    Support more than filenames—accept strings, buffers, streams, and even URLs. This ensures the parser works seamlessly in both Node.js and browsers, and fits small scripts as well as large-scale apps.

    Parsing Edge Cases:
    Handle real-world quirks like custom delimiters, quoted or multiline fields, escaped quotes, BOMs, empty lines, and comment lines. These features make the parser reliable across CSVs generated by different tools.

    Schema Handling:
    Give developers control over headers (auto-detect, custom names, case normalization) and allow type coercion (numbers, booleans, dates). Combine this with TypeScript generics so rows can be strongly typed.

    Error Handling:
    Provide clear strategies for malformed rows (throw, skip, or pad) and include detailed error messages with row/column numbers. This makes debugging and recovery much easier.

- #### Step 3: use an LLM to help expand your perspective.

    Include a list of the top 4 enhancements or edge cases you think are most valuable to explore in the next week’s sprint. Label them clearly by category (extensibility vs. functionality), and include whether they came from you, the LLM, or both. Describe these using the User Story format—see below for a definition. 

    Include your notes from above: what were your initial ideas, what did the LLM suggest, and how did the results differ by prompt? What resonated with you, and what didn’t? (3-5 sentences.) 

    Functionality:

    One: Handle Special CSV Cases
    User Story: “As a developer, I want the parser to correctly read rows that have quotes, commas, newlines, or empty spots so the data always comes out clean and correct.”

    Acceptance Criteria:
    - Quoted fields (like "Providence, RI") are parsed as one value.
    - Empty fields stay as empty strings or null.
    - Escaped quotes ("") are handled correctly.

    Two: Smarter Data Types & Headers
    User Story: “As a developer, I want the parser to turn values into numbers, booleans, or dates (when I choose) and let me set or auto-detect column names, so I don’t have to fix everything myself.”

    Acceptance Criteria:
    - Optional type conversion for numbers, booleans, and dates.
    - Support for custom or auto-detected headers.
    - Strong typing in TypeScript with generics.

    Extensibility:

    Three: More Ways to Input Data
    User Story: “As a developer, I want to give the parser data from files, strings, streams, or URLs so I can use it in different apps and environments.”

    Acceptance Criteria:
    - Accepts filenames, strings, buffers, or streams.
    - Works with async iteration (helpful for remote or big files).

    Four: Clearer Error Messages
    User Story: “As a developer, I want error messages that tell me exactly where the problem is in the CSV so I can fix it quickly.”

    Acceptance Criteria:
    - Errors show row and column numbers.
    - Options for handling bad rows (throw, skip, or pad with null).
    - Error messages explain the issue clearly.

    My first ideas focused on making sure the parser handles data correctly, like commas inside quotes, different data types, edge cases with empty fields, and giving clear error messages. The LLM agreed on edge cases and errors but also suggested bigger improvements, like supporting more input types (files, streams, URLs) and adding schema handling for headers and type conversion. My list was more about simplicity while the LLM’s was about flexibility and scalability. We both highlighted specific error handling, but some details like BOM handling felt less important right now.

### Design Choices

    Correctness
    A CSV parser is correct if it splits rows into the right number of fields, handles special cases, and handles empty lines without breaking. When a schema is provided, it should reject rows that don’t match the expected types. Without a schema, it should always return raw string arrays. Tests should check that invalid rows trigger errors and valid rows produce the right structure.

    Random, On-Demand Generation
    If I had a random CSV generator, I’d use it to create many different rows and files automatically. This would help test unusual cases I might forget, like missing values or very long strings. I could then check that the parser always returns arrays of consistent length and throws errors when data is buggy.

    Overall Experience, Bugs Encountered and Resolved
    This sprint was a good introduction to writing tests, creating files with data, and getting used to schemas. I wanted to test with different data tables and write tests in a neat and efficient way. I mostly had trouble with writing tests and the logic behind passing and failing them. For example, when Bob, thirty was tested to throw an exception, I focused on making sure the test passed, indicating that the error was thrown and the test was running as it should have. After playing around with the given code and then creating my own tests, I was able to pass all of them with the correct logic to back it up, although it did take me a while to figure out. 

    Design
    I designed parseCSV to be straightforward while still handling the main needs of a CSV parser. Inside the loop, I split each line on commas and trim the spaces, which covers most normal CSV formatting (for now). I added a quick check to skip over empty lines, since those are common in real files and shouldn’t cause errors. If no schema is passed in, the function pushes the raw array of strings to the result. If a schema is provided, the code calls schema.safeParse(values) to validate and transform the row. If the row is valid, it gets added as an object; if not, the function throws an error with the exact line number and validation message, so the caller knows exactly what went wrong and where. These choices keep the code short, clear, and flexible. It works for simple CSV parsing for the first sprint.


### 1340 Supplement

- #### 1. Correctness

- #### 2. Random, On-Demand Generation

- #### 3. Overall experience, Bugs encountered and resolved
#### Errors/Bugs:
#### Tests:
#### How To…

#### Team members and contributions (include cs logins): 
    N/A     
#### Collaborators (cslogins of anyone you worked with on this project and/or generative AI):
    I worked with GitHub Copilot to help me brainstorm some of the written perspective expansion prompts. I also asked it to help me brainstorm test cases and what data I should create.
#### Total estimated time it took to complete project: 
    It took me 3 total hours to complete this project
#### Link to GitHub Repo: 
    https://github.com/cs0320-f25/typescript-csv-lenawsong
